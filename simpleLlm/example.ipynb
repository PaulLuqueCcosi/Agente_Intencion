{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-llms-gemini\n",
      "  Downloading llama_index_llms_gemini-0.3.5-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-llms-gemini) (0.11.10)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.2.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-llms-gemini) (10.4.0)\n",
      "Collecting google-generativeai<0.6.0,>=0.5.2\n",
      "  Using cached google_generativeai-0.5.4-py3-none-any.whl (150 kB)\n",
      "Collecting google-api-core\n",
      "  Using cached google_api_core-2.19.2-py3-none-any.whl (139 kB)\n",
      "Collecting google-auth>=2.15.0\n",
      "  Using cached google_auth-2.34.0-py2.py3-none-any.whl (200 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.4\n",
      "  Using cached google_ai_generativelanguage-0.6.4-py3-none-any.whl (679 kB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.146.0-py2.py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.9.2)\n",
      "Requirement already satisfied: tqdm in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.66.5)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 KB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 KB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.6.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (8.5.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.10.5)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.0.8)\n",
      "Requirement already satisfied: dataclasses-json in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.6.7)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.9.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.2.14)\n",
      "Requirement already satisfied: wrapt in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2024.9.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.9.1)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.7.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.3)\n",
      "Requirement already satisfied: httpx in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.27.2)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.0.35)\n",
      "Requirement already satisfied: numpy<2.0.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (6.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (6.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.11.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.3.1)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2024.9.11)\n",
      "Requirement already satisfied: click in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.4.2)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (2.23.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from pydantic->google-generativeai<0.6.0,>=0.5.2->llama-index-llms-gemini) (0.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (3.22.0)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Collecting uritemplate<5,>=3.0.1\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.3.1)\n",
      "Requirement already satisfied: anyio in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (4.5.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (0.14.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Using cached grpcio_status-1.66.1-py3-none-any.whl (14 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Using cached grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (24.1)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/paul/SeminarioTesis/llamaIndex/.venv/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama-index-llms-gemini) (1.2.2)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "  Using cached grpcio_status-1.66.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.5-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.4-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.65.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.64.3-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.64.1-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.64.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.63.2-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.63.0-py3-none-any.whl (14 kB)\n",
      "  Using cached grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai, llama-index-llms-gemini\n",
      "Successfully installed cachetools-5.5.0 google-ai-generativelanguage-0.6.4 google-api-core-2.19.2 google-api-python-client-2.146.0 google-auth-2.34.0 google-auth-httplib2-0.2.0 google-generativeai-0.5.4 googleapis-common-protos-1.65.0 grpcio-1.66.1 grpcio-status-1.62.3 httplib2-0.22.0 llama-index-llms-gemini-0.3.5 proto-plus-1.24.0 protobuf-4.25.5 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.1.4 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llama-index-llms-gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms.gemini import Gemini\n",
    "  \n",
    "GOOGLE_API_KEY = os.environ[\"GOOGLE_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completado normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "llm = Gemini(model=\"models/gemini-pro\")\n",
    "resp = llm.complete(\"Hola\")\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Idea principal\": \"Definición de función recursiva\",\n",
      "  \"Tema\": \"Programación, funciones recursivas\",\n",
      "  \"Objetivo\": \"Enseñar o preguntar\",\n",
      "  \"Intención\": \"Iniciar conversación y enseñar\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "message = \"¿Sabes cómo se define una función recursiva?\"\n",
    "formato = \"json\"\n",
    "\n",
    "promt = f'''\n",
    "Estás ayudando a identificar la estructura y la intención de un mensaje entre dos estudiantes, A y B. El estudiante A está enseñando a B sobre un tema. Cuando el estudiante A envía un mensaje, tu tarea es analizarlo y responder los siguientes puntos:\n",
    "\n",
    "Idea principal: ¿Cuál es el concepto central del mensaje de A?\n",
    "Tema: ¿De qué tema o área está hablando el estudiante A?\n",
    "Objetivo: ¿Qué quiere lograr A con este mensaje? (por ejemplo, enseñar, preguntar, corregir, aclarar).\n",
    "Intención: ¿Cuál es la intención específica de A en este mensaje? (por ejemplo, enseñar, abrir conversación, pedir explicación, corregir).\n",
    "Antes de dar tu respuesta, piensa paso a paso sobre cómo se construye el mensaje y proporciona una explicación clara usando la técnica Chain of Thought. A continuación se muestra un ejemplo:\n",
    "\n",
    "Ejemplo de mensaje de estudiante A:\n",
    "\n",
    "\"Hola, ¿tienes alguna duda sobre cómo resolver esta ecuación de segundo grado?\"\n",
    "\n",
    "Razonamiento paso a paso (Chain of Thought):\n",
    "\n",
    "El mensaje comienza con una pregunta, lo que indica que el estudiante A quiere iniciar una conversación o interacción.\n",
    "Menciona \"ecuación de segundo grado\", lo que implica que el tema es matemáticas, específicamente ecuaciones cuadráticas.\n",
    "A pregunta si el estudiante B tiene alguna duda, lo que sugiere que su objetivo es ayudar a B a resolver la ecuación.\n",
    "La estructura de la pregunta es amigable y busca facilitar la conversación, lo que sugiere que la intención principal de A es abrir conversación y estar disponible para enseñar.\n",
    "Respuesta final:\n",
    "\n",
    "Idea principal: Resolver ecuaciones de segundo grado.\n",
    "Tema: Matemáticas, ecuaciones cuadráticas.\n",
    "Objetivo: Ofrecer ayuda o asistencia.\n",
    "Intención: Abrir conversación y ofrecer asistencia.\n",
    "\n",
    "Ahora analiza el siguiente mensaje '{message}' y dame la respuesta en formato '{formato}'\n",
    "'''\n",
    "\n",
    "\n",
    "# print(promt)\n",
    "\n",
    "resp = llm.complete(prompt=promt)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Idea principal\": \"El verbo 'to be' cambia de forma según el pronombre que lo acompaña.\",\n",
      "  \"Tema\": \"Gramática inglesa, verbo 'to be'\",\n",
      "  \"Objetivo\": \"Enseñar\",\n",
      "  \"Intención\": \"Proporcionar información sobre la conjugación del verbo 'to be'\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "message = \"El verbo tobe cambia con cada pronombre con el que se usa\"\n",
    "formato = \"json\"\n",
    "\n",
    "promt = f'''\n",
    "Estás ayudando a identificar la estructura y la intención de un mensaje entre dos estudiantes, A y B. El estudiante A está enseñando a B sobre un tema. Cuando el estudiante A envía un mensaje, tu tarea es analizarlo y responder los siguientes puntos:\n",
    "\n",
    "Idea principal: ¿Cuál es el concepto central del mensaje de A?\n",
    "Tema: ¿De qué tema o área está hablando el estudiante A?\n",
    "Objetivo: ¿Qué quiere lograr A con este mensaje? (por ejemplo, enseñar, preguntar, corregir, aclarar).\n",
    "Intención: ¿Cuál es la intención específica de A en este mensaje? (por ejemplo, enseñar, abrir conversación, pedir explicación, corregir).\n",
    "Antes de dar tu respuesta, piensa paso a paso sobre cómo se construye el mensaje y proporciona una explicación clara usando la técnica Chain of Thought. A continuación se muestra un ejemplo:\n",
    "\n",
    "Ejemplo de mensaje de estudiante A:\n",
    "\n",
    "\"Hola, ¿tienes alguna duda sobre cómo resolver esta ecuación de segundo grado?\"\n",
    "\n",
    "Razonamiento paso a paso (Chain of Thought):\n",
    "\n",
    "El mensaje comienza con una pregunta, lo que indica que el estudiante A quiere iniciar una conversación o interacción.\n",
    "Menciona \"ecuación de segundo grado\", lo que implica que el tema es matemáticas, específicamente ecuaciones cuadráticas.\n",
    "A pregunta si el estudiante B tiene alguna duda, lo que sugiere que su objetivo es ayudar a B a resolver la ecuación.\n",
    "La estructura de la pregunta es amigable y busca facilitar la conversación, lo que sugiere que la intención principal de A es abrir conversación y estar disponible para enseñar.\n",
    "Respuesta final:\n",
    "\n",
    "Idea principal: Resolver ecuaciones de segundo grado.\n",
    "Tema: Matemáticas, ecuaciones cuadráticas.\n",
    "Objetivo: Ofrecer ayuda o asistencia.\n",
    "Intención: Abrir conversación y ofrecer asistencia.\n",
    "\n",
    "Ahora analiza el siguiente mensaje '{message}' y dame la respuesta en formato '{formato}'\n",
    "'''\n",
    "\n",
    "\n",
    "# print(promt)\n",
    "\n",
    "resp = llm.complete(prompt=promt)\n",
    "print(resp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A backpack worn, a simple thing,\n",
      "But holds a magic, makes it sing.\n",
      "With every zip, a whispered rhyme,\n",
      "Unleashes wonders, space and time.\n",
      "\n",
      "A feather light, it seems to float,\n",
      "Though filled with treasures, a heavy boat.\n",
      "A book of spells, a map untold,\n",
      "A compass pointing, stories bold.\n",
      "\n",
      "A single touch, a whispered plea,\n",
      "And out it pours, a symphony.\n",
      "A shimmering gown, a crown of gold,\n",
      "A dragon's scale, a tale to be told.\n",
      "\n",
      "A hidden pocket, deep and wide,\n",
      "Where dreams are stored, side by side.\n",
      "A wish for flight, a wish to roam,\n",
      "The backpack grants, a magic home.\n",
      "\n",
      "So wear it well, this wondrous pack,\n",
      "And let its magic, guide you back.\n",
      "To worlds unseen, and lands unknown,\n",
      "Where every adventure, is your own. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "resp = Gemini().complete(\"Write a poem about a magic backpack\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completar chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Ahoy there! To help me suggest the perfect dinner, tell me a bit about what you're in the mood for:\n",
      "\n",
      "* **What kind of cuisine are you craving?** (Italian, Mexican, Asian, etc.)\n",
      "* **What ingredients do you have on hand?** (Any leftovers, fresh produce, etc.)\n",
      "* **How much time do you have to cook?** (Quick and easy or something more elaborate?)\n",
      "* **Are you cooking for yourself or others?** (If so, any dietary restrictions?)\n",
      "\n",
      "Once I have a better idea of your preferences, I can suggest some delicious dinner options! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "messages = [\n",
    "    ChatMessage(role=\"user\", content=\"Hello friend!\"),\n",
    "    ChatMessage(role=\"assistant\", content=\"Yarr what is shakin' matey?\"),\n",
    "    ChatMessage(\n",
    "        role=\"user\", content=\"Help me decide what to have for dinner.\"\n",
    "    ),\n",
    "]\n",
    "resp = Gemini().chat(messages)\n",
    "print(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
